{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query=\"What are all the education qualification of shyam?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def process_pdf_batch(pdf_file):\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pages = loader.load_and_split()\n",
    "    character_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=0\n",
    "        )\n",
    "    pages = character_splitter.split_documents(pages)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=\"resume\"\n",
    "chunks=process_pdf_batch(fr\"data\\{d}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=chunks, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Shyam Sundar\\n5,kavimani steet, Pankajam colony, 3rd cross street, Madurai, India - 625009\\n♂phone+91-9080765574 /envel⌢pemailshyamsundar.2022@gmail.com /linkedinshyamsundar007 /githubShyam-Sundar-7♂laptopPortfolio\\nProjects\\nLLM-Powered Coupon Recommender /github/video |Python, Streamlit, Langchain, OpenAI November 2023\\n•Developed a QA system for e-commerce with personalized coupon recommendations using OpenAI’s LLMs.\\n•Streamlined user interactions through a Streamlit interface and Langchain for real-world scenario simulations.\\n•Incorporated FAISS for refined recommendation processes.\\nPeopleCare Insurance Prediction /github |Python, Jupyter, Azure Cloud, Flask, Docker October 2023\\n•Expanded PeopleCare into vehicle insurance with a predictive model for effective customer targeting.\\n•Thorough analysis of customer behavior and data cleaning for accurate predictive modeling.\\n•Achieved 80% prediction accuracy using LightGBM.', metadata={'page': 0, 'source': 'data\\\\resume.pdf'}),\n",
       " Document(page_content='Shyam Sundar\\n5,kavimani steet, Pankajam colony, 3rd cross street, Madurai, India - 625009\\n♂phone+91-9080765574 /envel⌢pemailshyamsundar.2022@gmail.com /linkedinshyamsundar007 /githubShyam-Sundar-7♂laptopPortfolio\\nProjects\\nLLM-Powered Coupon Recommender /github/video |Python, Streamlit, Langchain, OpenAI November 2023\\n•Developed a QA system for e-commerce with personalized coupon recommendations using OpenAI’s LLMs.\\n•Streamlined user interactions through a Streamlit interface and Langchain for real-world scenario simulations.\\n•Incorporated FAISS for refined recommendation processes.\\nPeopleCare Insurance Prediction /github |Python, Jupyter, Azure Cloud, Flask, Docker October 2023\\n•Expanded PeopleCare into vehicle insurance with a predictive model for effective customer targeting.\\n•Thorough analysis of customer behavior and data cleaning for accurate predictive modeling.\\n•Achieved 80% prediction accuracy using LightGBM.', metadata={'page': 0, 'source': 'data\\\\resume.pdf'}),\n",
       " Document(page_content='Relevant Coursework\\n•Data Structures\\n•Advanced Numerical Techniques•Machine Learning\\n•Data Science•Deep Learning\\n•Computer Graphics\\nTechnical Skills\\nLanguages : Python, C/C++, SQL (Postgres), Matlab, Latex\\nFrameworks : Pytorch, Tensorflow, Flask, Pytorch Lightning\\nTools/Platform : Tableau, Power Bi, Azure, Git, Jupyter, Docker\\nLibraries : Scikit-Learn, Pandas, Numpy, Matplotlib, Seaborn\\nCertifications\\n•Certified Associate Data Analyst\\n•SQL [Advanced] - Hackerank\\n•Management Consulting Mentorship\\n•Generative AI at SAP•ML for Business professionals using No-Code AI tools\\n•Python [Basic] - Hackerank\\n•Software Engineer Intern -Hackerank\\nPublications\\nSuspicious Event Detection of Cargo Vessels Based on AIS Data at ICDMAI,2023', metadata={'page': 0, 'source': 'data\\\\resume.pdf'}),\n",
       " Document(page_content='Relevant Coursework\\n•Data Structures\\n•Advanced Numerical Techniques•Machine Learning\\n•Data Science•Deep Learning\\n•Computer Graphics\\nTechnical Skills\\nLanguages : Python, C/C++, SQL (Postgres), Matlab, Latex\\nFrameworks : Pytorch, Tensorflow, Flask, Pytorch Lightning\\nTools/Platform : Tableau, Power Bi, Azure, Git, Jupyter, Docker\\nLibraries : Scikit-Learn, Pandas, Numpy, Matplotlib, Seaborn\\nCertifications\\n•Certified Associate Data Analyst\\n•SQL [Advanced] - Hackerank\\n•Management Consulting Mentorship\\n•Generative AI at SAP•ML for Business professionals using No-Code AI tools\\n•Python [Basic] - Hackerank\\n•Software Engineer Intern -Hackerank\\nPublications\\nSuspicious Event Detection of Cargo Vessels Based on AIS Data at ICDMAI,2023', metadata={'page': 0, 'source': 'data\\\\resume.pdf'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def generate_queries():\n",
    "        \n",
    "    # Multi Query: Different Perspectives\n",
    "    template = \"\"\"You are an AI language model assistant. Your task is to generate Four \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "    prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "    generate_querie = (\n",
    "        prompt_perspectives \n",
    "        | ChatOpenAI(temperature=0) \n",
    "        | StrOutputParser() \n",
    "        | (lambda x: x.split(\"\\n\"))\n",
    "    )\n",
    "    return generate_querie \n",
    "\n",
    "\n",
    "def _get_docs(m):\n",
    "    docs=[]\n",
    "    docs.extend(m['original'])\n",
    "    for i in m['new']:\n",
    "        docs.extend(i)\n",
    "    return docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "original_question= retriever\n",
    "retrieval_chain =  generate_queries() | retriever.map()\n",
    "map_chain = RunnableParallel(original=original_question,new=retrieval_chain) | _get_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shyams\\Downloads\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\shyams\\Downloads\\RAG\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shyams\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\shyams\\Downloads\\RAG\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shyams\\Downloads\\RAG\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "embedding_function = SentenceTransformerEmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def project_embeddings(embeddings, umap_transform):\n",
    "    umap_embeddings = np.empty((len(embeddings),2))\n",
    "    for i, embedding in enumerate(tqdm(embeddings)): \n",
    "        umap_embeddings[i] = umap_transform.transform([embedding])\n",
    "    return umap_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "import numpy as np\n",
    "from pypdf import PdfReader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def _read_pdf(filename):\n",
    "    reader = PdfReader(filename)\n",
    "    \n",
    "    pdf_texts = [p.extract_text().strip() for p in reader.pages]\n",
    "\n",
    "    # Filter the empty strings\n",
    "    pdf_texts = [text for text in pdf_texts if text]\n",
    "    return pdf_texts\n",
    "\n",
    "\n",
    "def _chunk_texts(texts):\n",
    "    character_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=0\n",
    "    )\n",
    "    character_split_texts = character_splitter.split_text('\\n\\n'.join(texts))\n",
    "\n",
    "    token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
    "\n",
    "    token_split_texts = []\n",
    "    for text in character_split_texts:\n",
    "        token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "    return token_split_texts\n",
    "\n",
    "def load_chroma(filename, collection_name, embedding_function):\n",
    "    texts = _read_pdf(filename)\n",
    "    chunks = _chunk_texts(texts)\n",
    "\n",
    "    chroma_cliet = chromadb.Client()\n",
    "    chroma_collection = chroma_cliet.create_collection(name=collection_name, embedding_function=embedding_function)\n",
    "\n",
    "    ids = [str(i) for i in range(len(chunks))]\n",
    "\n",
    "    chroma_collection.add(ids=ids, documents=chunks)\n",
    "\n",
    "    return chroma_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_chroma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[43mload_chroma\u001b[49m(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mesume.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m, collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m'\u001b[39m, embedding_function\u001b[38;5;241m=\u001b[39membedding_function)\n\u001b[0;32m      2\u001b[0m chroma_collection\u001b[38;5;241m.\u001b[39mcount()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_chroma' is not defined"
     ]
    }
   ],
   "source": [
    "chroma_collection = load_chroma(filename='data\\resume.pdf', collection_name='resume', embedding_function=embedding_function)\n",
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = chroma_collection.get(include=['embeddings'])['embeddings']\n",
    "umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(embeddings)\n",
    "projected_dataset_embeddings = project_embeddings(embeddings, umap_transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
